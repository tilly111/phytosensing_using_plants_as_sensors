{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Phytosensing – Using Plants as Sensors\n",
    "## Introduction\n",
    "In this tutorial we will develop a toolchain to analyse and classify plant electrical signals to infer the external stimulus of the given plant.\n",
    "Because collecting the data would take too much time we will use a dataset that we collected earlier (in Lübeck). It is a real world dataset.\n",
    "I planned the practical in the way that you will be guided step by step. I encourage the peoples who are familiar with programming in python (and the used libraries) to explore other possible approaches. In the end we will do a little challenge were you can implement a classifier (and possible pipeline) and the best performing classifier will win (maybe a prize maybe not).\n",
    "\n",
    "## 1. Load the dataset (data acquisition)\n",
    "Because we do not have to collect a dataset this section is only about loading data, visualization, and cutting the data into the appropriate experiment intervals. Hence we will do the following steps:\n",
    "\n",
    "1. Load the dataset\n",
    "2. Visualize the dataset\n",
    "3. cut the dataset into the appropriate length\n",
    "\n",
    "<em>Hint: for the final challenge you are allowed to use all available data (not only the cut data)</em>\n",
    "\n",
    "### Import necessary libraries\n",
    "These libraries are necessary for the tutorial. You can add more libraries if you want to use them later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import resample\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "# from keras.optimizers import Adam\n",
    "from keras.optimizers.legacy import Adam\n",
    "from sklearn import preprocessing\n",
    "from numpy import genfromtxt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset\n",
    "The data were collected using the Cybres MU measurement system. Hence, we have a lot of information that are not interesting for us. We will only use the following columns: `timestamp`, `differential_potential_CH1`, and `differential_potential_CH2`. In a first step load the data into a pandas dataframe and print the first 5 rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load the data and print the first 5 rows\n",
    "wind_1 = pd.read_csv('data/raw_data/wind/series_1.csv')\n",
    "print(wind_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### reduce the dataset to the necessary information\n",
    "As you see, the raw data contain much information we do not need (in this tutorial). So next step is to drop all but the relevant colums."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop all but the relevant columns\n",
    "wind_1 = wind_1[['timestamp', 'differential_potential_CH1', 'differential_potential_CH2']]\n",
    "# reset the index\n",
    "wind_1 = wind_1.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the dataset\n",
    "After you successfully removed unnecessary information, you can have a first look at the data. Please plot the two electrical signals over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wind_1.plot(x='timestamp', y=['differential_potential_CH1', 'differential_potential_CH2'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cut the dataset into the appropriate length\n",
    "After plotting the data, we now want to cut them according to the experiments. The required timestamps are given in `experiment_start.csv` and `experiment_end.csv`. Load these files and cut the data into the appropriate intervals. For future processing you may want to save the intermediate results that you do not have to compute them again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stimulus = \"temperature\"\n",
    "# load the experiment start timestamps\n",
    "start_times = pd.read_csv(f\"data/{stimulus}_experiment_start.csv\", parse_dates=True)\n",
    "\n",
    "running_exp_number = 0\n",
    "for i in range(1, 5):\n",
    "    # transform the timestamps to datetime\n",
    "    start_times[f'series_{i}'] = pd.to_datetime(start_times[f'series_{i}'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    # load data\n",
    "    wind = pd.read_csv(f\"data/raw_data/{stimulus}/series_{i}.csv\")\n",
    "    wind[\"timestamp\"] = pd.to_datetime(wind[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # iterate through data and cut\n",
    "    for j in range(start_times.shape[0]):\n",
    "        begin = start_times[f'series_{i}'].iloc[j]\n",
    "        # because series have different amounts of experiments\n",
    "        if begin is not pd.NaT:\n",
    "            end = begin + timedelta(minutes=130)\n",
    "            print(f\"begin: {begin} -- end: {end}\")\n",
    "            tmp_wind = wind.loc[((wind[\"timestamp\"] >= begin) & (wind[\"timestamp\"] <= end))]\n",
    "\n",
    "            tmp_wind = tmp_wind.reset_index(drop=True)\n",
    "\n",
    "            tmp_wind = tmp_wind[['timestamp', 'differential_potential_CH1', 'differential_potential_CH2']]\n",
    "            # plot\n",
    "            plt.plot(tmp_wind[\"timestamp\"], tmp_wind[\"differential_potential_CH1\"], label=\"differential_potential_CH1\")\n",
    "            plt.plot(tmp_wind[\"timestamp\"], tmp_wind[\"differential_potential_CH2\"], label=\"differential_potential_CH2\")\n",
    "            plt.axvspan(tmp_wind[\"timestamp\"].iloc[0]+timedelta(minutes=60), tmp_wind[\"timestamp\"].iloc[0] + timedelta(minutes=70), facecolor='0.2', alpha=0.5)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            # save back to csv\n",
    "            tmp_wind.to_csv(f\"data/cut_data/{stimulus}/experiment_{running_exp_number}.csv\", index=False)\n",
    "            running_exp_number += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cut data continued\n",
    "Well done. By now you should have a folder of files, were every file contains one experiment. Now you have to repeat it for all stimuli and series. This can be done by using a for loop. If you are short on time you can skip this step and use the provided cut data (data/cut_data/*)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cut all the data\n",
    "# ATTENTION if you want to cut the light data you have to use a different approach (because the experimental protocol is slightly different\n",
    "# # blue light: starting at 1 am; every 6 hours\n",
    "# 1 am, 7 am 1 pm, 7 pm\n",
    "# red light: starting at 4 am; every 6 hours\n",
    "# 4 am, 10 am, 4 pm, 10 pm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaling\n",
    "While cutting the data you probably recognised that the values are quite large. This is because they have not yet been scaled. Currently, the data are raw values from the measurement unit. You can convert them using the following formula:\n",
    "$$ x \\text{mV} = \\frac{x_{\\text{raw}} - 512000}{1000} $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the electric potential channels to mV\n",
    "def scale_data(data):\n",
    "    data['differential_potential_CH1'] = (data['differential_potential_CH1'] - 512000) / 1000\n",
    "    data['differential_potential_CH2'] = (data['differential_potential_CH2'] - 512000) / 1000\n",
    "    return data\n",
    "\n",
    "for i in range(len(os.listdir(\"data/cut_data/wind\"))):\n",
    "    print(i)\n",
    "    df = pd.read_csv(f\"data/cut_data/wind/experiment_{i}.csv\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # scaling\n",
    "    df = scale_data(df)\n",
    "\n",
    "    # plot\n",
    "    plt.plot(df[\"timestamp\"], df[\"differential_potential_CH1\"], label=\"differential_potential_CH1\")\n",
    "    plt.plot(df[\"timestamp\"], df[\"differential_potential_CH2\"], label=\"differential_potential_CH2\")\n",
    "    plt.axvspan(df[\"timestamp\"].iloc[0]+timedelta(minutes=60), df[\"timestamp\"].iloc[0] + timedelta(minutes=70), facecolor='0.2', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Feature extraction\n",
    "Now that we have the data in the correct format, we can start with the feature extraction. We begin by extracting features from the stimuli part and the initial 10 minutes for background subtraction. We will calculate the mean, std, and average power spectral density. (Attention some samples may contain NaN values/are incomplete!)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def power_spectral_density(data):\n",
    "    # Calculate PSD using welch estimate. PSD gives the spectra\n",
    "    # depending on the frequencies. Sum over all spectra to\n",
    "    # receive the total (average) spectral power\n",
    "    _, psd = welch(data)  # TODO adjust parameters such as sampling frequency\n",
    "    return sum(psd)\n",
    "\n",
    "\n",
    "mean_ch1 = []\n",
    "mean_ch2 = []\n",
    "std_ch1 = []\n",
    "std_ch2 = []\n",
    "asp_ch1 = []\n",
    "asp_ch2 = []\n",
    "\n",
    "stimulus = \"wind\"\n",
    "\n",
    "nb_experiments = len(os.listdir(f\"data/scaled_data/{stimulus}\"))\n",
    "\n",
    "for i in range(nb_experiments):\n",
    "    # print(f\"loading experiment {i}\")\n",
    "    df = pd.read_csv(f\"data/scaled_data/{stimulus}/experiment_{i}.csv\")\n",
    "    # deselect empty experiments\n",
    "    if df.empty:\n",
    "        print(f\"experiment {i} is empty\")\n",
    "        continue\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # select stimulus phase and background subtraction phase\n",
    "    stim = df[((df[\"timestamp\"] >= df[\"timestamp\"][0] + timedelta(minutes=60)) & (\n",
    "            df[\"timestamp\"] <= df[\"timestamp\"][0] + timedelta(minutes=70)))]\n",
    "    bg = df[((df[\"timestamp\"] >= df[\"timestamp\"][0] + timedelta(minutes=0)) & (\n",
    "            df[\"timestamp\"] <= df[\"timestamp\"][0] + timedelta(minutes=10)))]\n",
    "\n",
    "    # calculate features per channels\n",
    "    # make sure the values can be calculated\n",
    "    if stim[\"differential_potential_CH1\"].mean() is np.NAN or bg[\"differential_potential_CH1\"].mean() is np.NAN:\n",
    "        print(f\"experiment {i} has NaNs\")\n",
    "        continue\n",
    "    # calculate mean\n",
    "    mean_ch1.append(stim[\"differential_potential_CH1\"].mean() - bg[\"differential_potential_CH1\"].mean())\n",
    "    mean_ch2.append(stim[\"differential_potential_CH2\"].mean() - bg[\"differential_potential_CH2\"].mean())\n",
    "\n",
    "    # calculate std\n",
    "    std_ch1.append(stim[\"differential_potential_CH1\"].std() - bg[\"differential_potential_CH1\"].std())\n",
    "    std_ch2.append(stim[\"differential_potential_CH2\"].std() - bg[\"differential_potential_CH2\"].std())\n",
    "\n",
    "    # calculate asp\n",
    "    asp_ch1.append(power_spectral_density(stim[\"differential_potential_CH1\"]) - power_spectral_density(bg[\"differential_potential_CH1\"]))\n",
    "    asp_ch2.append(power_spectral_density(stim[\"differential_potential_CH2\"]) - power_spectral_density(bg[\"differential_potential_CH2\"]))\n",
    "\n",
    "df = pd.DataFrame({\"mean_ch1\": mean_ch1, \"mean_ch2\": mean_ch2, \"std_ch1\": std_ch1, \"std_ch2\": std_ch2, \"asp_ch1\": asp_ch1, \"asp_ch2\": asp_ch2})\n",
    "df.to_csv(f\"data/features/{stimulus}/mean_std_asp_features.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extraction continued\n",
    "I encourage you to find and implement other features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO implement other interesting features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data augmentation (Slicing)\n",
    "This is voluntary. If you do not plan on using deep learning you can skip this part. If you just want to do deep learning, you can use just the raw time series. However, as you may notice, the dataset is rather small and probably not sufficient for deep learning. Hence, we can use data augmentation strategies to enlarge the dataset. A starting point can be the [tsaug library](url https://tsaug.readthedocs.io/en/stable/). It provides a lot of different data augmentation strategies for time series data. Some simple example is to add white noise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stimulus = \"temperature\"\n",
    "n_experiments = len(os.listdir(f\"data/scaled_data/{stimulus}\"))\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    # print(f\"experiment {i}\")\n",
    "    df = pd.read_csv(f\"data/scaled_data/{stimulus}/experiment_{i}.csv\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"skipping experiment {i} because of empty data\")\n",
    "        continue\n",
    "\n",
    "    # get timings\n",
    "    stimulus_application_begin = df[\"timestamp\"].iloc[0] + timedelta(minutes=59)\n",
    "    stimulus_application_end = df[\"timestamp\"].iloc[0] + timedelta(minutes=60+11)  # 10 min stimulus\n",
    "\n",
    "    # cut data\n",
    "    df = df[df[\"timestamp\"] >= stimulus_application_begin]\n",
    "    df = df[df[\"timestamp\"] <= stimulus_application_end]\n",
    "\n",
    "    new_num_points = 418\n",
    "    tmp_df = df.set_index('timestamp')\n",
    "    if tmp_df.shape[0] < new_num_points-10:\n",
    "        print(f\"skipping experiment {i} because of too few data points\")\n",
    "        continue\n",
    "    interpolated_df = resample(tmp_df, num=new_num_points)\n",
    "\n",
    "    ch_1 = np.reshape(interpolated_df[:, 0], (1, -1))\n",
    "    ch_2 = np.reshape(interpolated_df[:, 1], (1, -1))\n",
    "\n",
    "    # TODO augment data! (e.g. add white noise)\n",
    "    if i == 0:\n",
    "        ch_1_all = ch_1\n",
    "        ch_2_all = ch_2\n",
    "    else:\n",
    "        ch_1_all = np.vstack((ch_1_all, ch_1))\n",
    "        ch_2_all = np.vstack((ch_2_all, ch_2))\n",
    "\n",
    "    # plot data\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    # ax.plot(df[\"timestamp\"], df[\"differential_potential_CH1\"], label=\"differential_potential_CH1\")\n",
    "    # ax.plot(df[\"timestamp\"], df[\"differential_potential_CH2\"], label=\"differential_potential_CH2\")\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    # ax.plot(interpolated_df[:, 0], label=\"differential_potential_CH1\")\n",
    "    # ax.plot(interpolated_df[:, 1], label=\"differential_potential_CH2\")\n",
    "    # ax.set_xlabel(\"Time\")\n",
    "    # ax.set_ylabel(\"Voltage [mV]\")\n",
    "    # ax.legend()\n",
    "    # plt.show()\n",
    "\n",
    "print(ch_1_all.shape)\n",
    "print(ch_2_all.shape)\n",
    "\n",
    "df = pd.DataFrame(ch_1_all)\n",
    "df.to_csv(f\"data/dataset/{stimulus}/ch_1.csv\", index=False, header=False)\n",
    "df = pd.DataFrame(ch_2_all)\n",
    "df.to_csv(f\"data/dataset/{stimulus}/ch_2.csv\", index=False, header=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the classification data set\n",
    "Now, that we have features extracted, we need to prepare the data for classification by adding class information and splitting the data in training and test set.\n",
    "\n",
    "#### Add class information\n",
    "Here we structure the data into feature vectors and the corresponding class labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wind_features = pd.read_csv(\"data/features/wind/mean_std_asp_features.csv\")\n",
    "temperature_features = pd.read_csv(\"data/features/temperature/mean_std_asp_features.csv\")\n",
    "# red_features = pd.read_csv(\"data/features/red_light/mean_std_asp_features.csv\")\n",
    "# blue_features = pd.read_csv(\"data/features/blue_light/mean_std_asp_features.csv\")\n",
    "#\n",
    "# mean_wind = wind_features[\"mean_ch1\"].tolist()\n",
    "# mean_wind.extend(wind_features[\"mean_ch2\"].tolist())\n",
    "# std_wind = wind_features[\"std_ch1\"].tolist()\n",
    "# std_wind.extend(wind_features[\"std_ch2\"].tolist())\n",
    "# asp_wind = wind_features[\"asp_ch1\"].tolist()\n",
    "# asp_wind.extend(wind_features[\"asp_ch2\"].tolist())\n",
    "# wind_features = pd.DataFrame({\"mean\": mean_wind, \"std\": std_wind, \"asp\": asp_wind})\n",
    "#\n",
    "# mean_temperature = temperature_features[\"mean_ch1\"].tolist()\n",
    "# mean_temperature.extend(temperature_features[\"mean_ch2\"].tolist())\n",
    "# std_temperature = temperature_features[\"std_ch1\"].tolist()\n",
    "# std_temperature.extend(temperature_features[\"std_ch2\"].tolist())\n",
    "# asp_temperature = temperature_features[\"asp_ch1\"].tolist()\n",
    "# asp_temperature.extend(temperature_features[\"asp_ch2\"].tolist())\n",
    "# temperature_features = pd.DataFrame({\"mean\": mean_wind, \"std\": std_wind, \"asp\": asp_wind})\n",
    "\n",
    "all_feature_names = wind_features.columns.to_list()\n",
    "wind_features = wind_features.to_numpy()\n",
    "temperature_features = temperature_features.to_numpy()\n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(10)  # TODO set to reproduce results\n",
    "np.random.shuffle(wind_features)\n",
    "np.random.shuffle(temperature_features)\n",
    "\n",
    "# select random features from wind\n",
    "train_len = int(0.7 * wind_features.shape[0])\n",
    "test_len = int(0.3 * wind_features.shape[0])\n",
    "x_wind_train = wind_features[:train_len, :]\n",
    "x_wind_test = wind_features[train_len:, :]\n",
    "y_wind_train = np.zeros((x_wind_train.shape[0],))\n",
    "y_wind_test = np.zeros((x_wind_test.shape[0],))\n",
    "\n",
    "\n",
    "# select random features from temperature\n",
    "train_len = int(0.7 * temperature_features.shape[0])\n",
    "test_len = int(0.3 * temperature_features.shape[0])\n",
    "x_temperature_train = temperature_features[:train_len, :]\n",
    "x_temperature_test = temperature_features[train_len:, :]\n",
    "y_temperature_train = np.ones((x_temperature_train.shape[0],))\n",
    "y_temperature_test = np.ones((x_temperature_test.shape[0],))\n",
    "\n",
    "# combine all classes\n",
    "x_train = np.concatenate((x_wind_train, x_temperature_train), axis=0)\n",
    "y_train = np.concatenate((y_wind_train, y_temperature_train), axis=0)\n",
    "x_test = np.concatenate((x_wind_test, x_temperature_test), axis=0)\n",
    "y_test = np.concatenate((y_wind_test, y_temperature_test), axis=0)\n",
    "\n",
    "\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}, y_test: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Classification - Classical Machine Learning\n",
    "In this section we will use the previous generated dataset for classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "number_of_features = len(all_feature_names)\n",
    "\n",
    "# scaling the features based on the training set\n",
    "min_max = MinMaxScaler()\n",
    "for i in range(number_of_features):  # min max scale all the features\n",
    "    x_train[:, i] = min_max.fit_transform(np.reshape(x_train[:, i], (x_train[:, i].shape[0], 1))).squeeze()\n",
    "    x_test[:, i] = min_max.transform(np.reshape(x_test[:, i], (x_test[:, i].shape[0], 1))).squeeze()\n",
    "\n",
    "# make classifier and fit\n",
    "# estimator = SVC(kernel=\"poly\", probability=True)  # , class_weight='balanced'\n",
    "estimator = LinearDiscriminantAnalysis()\n",
    "selector = estimator.fit(x_train, y_train)\n",
    "y_pred = selector.predict(x_test)\n",
    "\n",
    "## SHAP for explainabiltiy\n",
    "# x_t = pd.DataFrame(data=x_test, columns=all_feature_names)\n",
    "# x_train_wn = pd.DataFrame(data=x_train, columns=all_feature_names)\n",
    "#\n",
    "# explainer = shap.KernelExplainer(selector.predict_proba, x_train_wn)\n",
    "# shap_value = explainer(x_t)\n",
    "# # returns probability for class 0 and 1, but we only need one bc p = 1 - p\n",
    "# shap_value.values = shap_value.values[:, :, 1]\n",
    "# shap_value.base_values = shap_value.base_values[:, 1]\n",
    "#\n",
    "# print_frame = pd.DataFrame(data=np.zeros((1, number_of_features)), columns=all_feature_names)\n",
    "# print_frame[:] = shap_value.abs.mean(axis=0).values\n",
    "# print(\"----------\")\n",
    "# for z in print_frame.columns:\n",
    "#     print(f\"{z}\\t{print_frame[z].to_numpy().squeeze()}\")\n",
    "#     # print(f\"{print_frame[z].to_numpy().squeeze()}\")\n",
    "# print(\"----------\")\n",
    "\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"confusion matrix: \\n{confusion_matrix(y_test, y_pred)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Classification - Deep Learning\n",
    "In this section we will use the previous generated dataset for classification using deep learning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "np.random.seed(10)  # TODO set to reproduce results\n",
    "split_ratio = 0.8\n",
    "batch_size = 32\n",
    "model_type = \"LSTM\"  # Dense or LSTM\n",
    "\n",
    "wind_ch1 = genfromtxt(\"data/dataset/wind/ch_1.csv\", delimiter=',')\n",
    "wind_ch2 = genfromtxt(\"data/dataset/wind/ch_2.csv\", delimiter=',')\n",
    "temperature_ch1 = genfromtxt(\"data/dataset/temperature/ch_1.csv\", delimiter=',')\n",
    "temperature_ch2 = genfromtxt(\"data/dataset/temperature/ch_2.csv\", delimiter=',')\n",
    "\n",
    "print(wind_ch1.shape, wind_ch2.shape, temperature_ch1.shape, temperature_ch2.shape)\n",
    "\n",
    "wind = np.concatenate((wind_ch1, wind_ch2), axis=0)\n",
    "# shuffle the data\n",
    "idx = np.random.permutation(wind.shape[0])\n",
    "wind = wind[idx, :]\n",
    "\n",
    "split_index = int(split_ratio * wind.shape[0])\n",
    "wind_train = wind[:split_index, :]\n",
    "wind_test = wind[split_index:, :]\n",
    "\n",
    "temperature = np.concatenate((temperature_ch1, temperature_ch2), axis=0)\n",
    "idx = np.random.permutation(temperature.shape[0])\n",
    "temperature = temperature[idx, :]\n",
    "\n",
    "split_index = int(split_ratio * temperature.shape[0])\n",
    "temperature_train = temperature[:split_index, :]\n",
    "temperature_test = temperature[split_index:, :]\n",
    "\n",
    "x_train = np.concatenate((wind_train, temperature_train), axis=0)\n",
    "y_train = np.concatenate((np.zeros((wind_train.shape[0],)), np.ones((temperature_train.shape[0],))))\n",
    "\n",
    "x_test = np.concatenate((wind_test, temperature_test), axis=0)\n",
    "y_test = np.concatenate((np.zeros((wind_test.shape[0],)), np.ones((temperature_test.shape[0],))))\n",
    "\n",
    "# make dim right for LSTM\n",
    "if model_type == \"LSTM\":\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# do one hot encoding to get the probability distribution\n",
    "enc = preprocessing.OneHotEncoder(categories='auto')\n",
    "enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "# You can blur the labels for better numerical stability\n",
    "# y_train[y_train == 1] -= (0.01 * (n_classes-1))\n",
    "# y_train[y_train == 0] += 0.01\n",
    "#\n",
    "# y_test[y_test == 1] -= (0.01 * (n_classes-1))\n",
    "# y_test[y_test == 0] += 0.01\n",
    "\n",
    "# Let's make our dataset performant using tf.data API\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "if model_type == \"LSTM\":\n",
    "    model.add(keras.layers.LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(keras.layers.LSTM(100))\n",
    "    model.add(keras.layers.Flatten())\n",
    "elif model_type == \"Dense\":\n",
    "    model.add(InputLayer(input_shape=(x_train.shape[1],), batch_size=batch_size))\n",
    "    # model.add(Dense(1000, activation='relu'))\n",
    "    # model.add(Dense(720, activation='relu'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='sigmoid'))  # see: https://stackoverflow.com/questions/49016723/softmax-cross-entropy-loss-explodes\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "print(f\"is gpu available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(model.summary())\n",
    "# fit model\n",
    "# save best model for later use\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"neural_network/best_{model_type}_model\",\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=100, batch_size=32,\n",
    "                    callbacks=[model_checkpoint_callback], verbose=True)\n",
    "\n",
    "# validate model -> use the best model\n",
    "model = tf.keras.models.load_model(f\"neural_network/best_{model_type}_model\")\n",
    "y_pred = model.predict(x_test, verbose=True)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# calculate metrics\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average='binary')  # average=None returns score for each class\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"F1 Score: {f1_s}\")\n",
    "print(f\"Confusion matrix: \\n{cm}\")\n",
    "\n",
    "# plotting the loss and accuracy\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "xc = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xc, train_loss, label=\"train loss\")\n",
    "plt.plot(xc, val_loss, label=\"val loss\")\n",
    "plt.plot(xc, train_acc, label=\"train accuracy\")\n",
    "plt.plot(xc, val_acc, label=\"val accuracy\")\n",
    "plt.title(f\"{model_type} loss and accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f\"plots/{model_type}_loss_acc_.pdf\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Challenge\n",
    "Now that you have seen the basic approaches, you can try to implement your own classifier. The best performing classifier will win a prize. You can use all resources! Best of luck"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO make the best classifier and win a prize"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
