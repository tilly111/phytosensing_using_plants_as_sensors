{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Phytosensing – Using Plants as Sensors\n",
    "## Introduction\n",
    "In this tutorial we will develop a toolchain to analyse and classify plant electrical signals to infer the external stimulus of the given plant.\n",
    "Because collecting the data would take too much time we will use a dataset that we collected earlier (in Lübeck). It is a real world dataset.\n",
    "I planned the practical in the way that you will be guided step by step. I encourage the people who are familiar with programming in python (and the used libraries) to explore other possible approaches. In the end we will do a little challenge were you can implement a classifier (and possible pipeline) and the best performing classifier will win.\n",
    "\n",
    "## 1. Load the dataset (data acquisition)\n",
    "Because we do not have to collect a dataset this step only consists of loading data, visualization, and cutting the data into the appropriate experiment intervals. Hence, we will do the following steps:\n",
    "\n",
    "1. Load the dataset;\n",
    "2. Visualize the dataset;\n",
    "3. Cut the dataset into the appropriate length; and\n",
    "4. Scale the data to real units.\n",
    "\n",
    "<em>Hint: for the final challenge you are allowed to use all available data (not only the cut data)</em>\n",
    "\n",
    "### Import necessary libraries\n",
    "These libraries are necessary for the tutorial. You can add more libraries if you want to use them later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "# from keras.optimizers import Adam\n",
    "from keras.optimizers.legacy import Adam\n",
    "from sklearn import preprocessing\n",
    "from numpy import genfromtxt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset\n",
    "The data were collected using the Cybres MU measurement system. Hence, we have a lot of information that are not interesting for us. We will only use the following columns: `timestamp`, `differential_potential_CH1`, and `differential_potential_CH2`. In a first step load the data into a pandas dataframe and print the first 5 rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load the data and print the first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### reduce the dataset to the necessary information\n",
    "As you see, the raw data contain much information we do not need (in this tutorial). Hence, the next step is to drop all but the relevant columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop all but the relevant columns\n",
    "\n",
    "# reset the index\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the dataset\n",
    "After you successfully removed unnecessary information, you can have a first look at the data. Please plot the two electrical signals over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot differnetial_potential_CH1 and differential_potential_CH2 over time\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cut the dataset into the appropriate length\n",
    "After plotting the data, we now want to cut the data according to the experiments. The required timestamps are given in `experiment_start.csv` and `experiment_end.csv`. Load these files and cut the data into the appropriate intervals. For future processing you may want to save the intermediate results that you do not have to compute them again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stimulus = \"wind\"\n",
    "# load the experiment start timestamps\n",
    "\n",
    "running_exp_number = 0\n",
    "for i in range(1, 5):\n",
    "    # transform the timestamps to datetime\n",
    "\n",
    "    # load data\n",
    "\n",
    "    # iterate through data and cut\n",
    "    for j in range(...):\n",
    "        begin_experiment = # TODO\n",
    "        # because series have different number of experiments\n",
    "        if begin_experiment is not pd.NaT:\n",
    "            end_experiment = begin_experiment + timedelta(minutes=...)\n",
    "            print(f\"begin: {begin_experiment} -- end: {end_experiment}\")\n",
    "            # cut data\n",
    "\n",
    "            # (optional) plot the cut data\n",
    "\n",
    "            # save back to csv\n",
    "\n",
    "            # increase experiment number\n",
    "            running_exp_number += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cut data continued\n",
    "Well done. By now you should have a folder of files, were every file contains one experiment. Now you have to repeat it for all stimuli and series. This can be done by using a `for` loop. If you are short on time you can skip this step and use the provided cut data (`data/cut_data/*`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cut all the data\n",
    "# ATTENTION if you want to cut the light data you have to use a different approach (because the experimental protocol is slightly different\n",
    "# # blue light: starting at 1 am; every 6 hours\n",
    "# 1 am, 7 am 1 pm, 7 pm\n",
    "# red light: starting at 4 am; every 6 hours\n",
    "# 4 am, 10 am, 4 pm, 10 pm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaling\n",
    "While cutting the data you probably recognised that the values are quite large. This is because they have not yet been scaled. Currently, the data are raw values from the measurement unit. You can convert them using the following formula:\n",
    "$$ x \\text{mV} = \\frac{x_{\\text{raw}} - 512000}{1000} $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the electric potential channels to mV\n",
    "def scale_data(data):\n",
    "    # TODO apply the above formula\n",
    "    return data\n",
    "\n",
    "# iterate through all experiments\n",
    "for i in range(...):\n",
    "    # load experiment\n",
    "\n",
    "    # scaling\n",
    "    df = scale_data(df)\n",
    "\n",
    "    # (optional) plot\n",
    "\n",
    "    # save back to csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Feature extraction\n",
    "Now that we have the data in the correct format, we can start with the feature extraction. We begin by extracting features from the stimuli part and the initial 10 minutes for background subtraction. We will calculate the mean, std, and average power spectral density. (Attention some samples may contain NaN values/are incomplete!)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def power_spectral_density(data):\n",
    "    # Calculate PSD using welch estimate. PSD gives the spectra\n",
    "    # depending on the frequencies. Sum over all spectra to\n",
    "    # receive the total (average) spectral power\n",
    "    _, psd = welch(data)\n",
    "    return sum(psd)\n",
    "\n",
    "\n",
    "stimulus = \"wind\"\n",
    "\n",
    "nb_experiments = ... # number of experiments\n",
    "\n",
    "for i in range(nb_experiments):\n",
    "    # load experiment\n",
    "    df = ...\n",
    "    # deselect empty experiments\n",
    "    if df.empty:\n",
    "        print(f\"experiment {i} is empty\")\n",
    "        continue\n",
    "\n",
    "    # convert timestamp to datetime\n",
    "\n",
    "    # select stimulus phase and background subtraction phase\n",
    "\n",
    "    # calculate features per channels\n",
    "    # make sure the values can be calculated (e.g. no NaN values)\n",
    "\n",
    "    # calculate mean\n",
    "\n",
    "    # calculate std\n",
    "\n",
    "    # calculate asp\n",
    "\n",
    "# load the features into a dataframe and save it to a csv file\n",
    "df = pd.DataFrame({\"mean_ch1\": ..., \"mean_ch2\": ..., \"std_ch1\": ..., \"std_ch2\": ..., \"asp_ch1\": ..., \"asp_ch2\": ...})\n",
    "df.to_csv(f\"data/features/{stimulus}/mean_std_asp_features.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extraction continued\n",
    "I encourage you to find and implement other features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO implement other interesting features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data augmentation (Slicing)\n",
    "This is voluntary. If you do not plan on using deep learning you can skip this part. If you just want to do deep learning, you can use just the raw time series. However, as you may notice, the dataset is rather small and probably not sufficient for deep learning. Hence, we can use data augmentation strategies to enlarge the dataset. A starting point can be the [tsaug library](https://tsaug.readthedocs.io/en/stable/). It provides a lot of different data augmentation strategies for time series data. Some simple example is to add white noise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stimulus = \"wind\"\n",
    "n_experiments = len(os.listdir(f\"data/scaled_data/{stimulus}\"))\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    # load experiment data\n",
    "    df = pd.read_csv(f\"data/scaled_data/{stimulus}/experiment_{i}.csv\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # make sure that the dataframe is not empty\n",
    "    if df.empty:\n",
    "        print(f\"skipping experiment {i} because of empty data\")\n",
    "        continue\n",
    "\n",
    "    # get timings\n",
    "    stimulus_application_begin = df[\"timestamp\"].iloc[0] + timedelta(minutes=59)\n",
    "    stimulus_application_end = df[\"timestamp\"].iloc[0] + timedelta(minutes=60+11)  # 10 min stimulus\n",
    "\n",
    "    # cut data\n",
    "    df = df[df[\"timestamp\"] >= stimulus_application_begin]\n",
    "    df = df[df[\"timestamp\"] <= stimulus_application_end]\n",
    "\n",
    "    # make sure all experiments have the same length -> resample\n",
    "    new_num_points = 418\n",
    "    tmp_df = df.set_index('timestamp')\n",
    "    if tmp_df.shape[0] < new_num_points-10:\n",
    "        print(f\"skipping experiment {i} because of too few data points\")\n",
    "        continue\n",
    "    interpolated_df = resample(tmp_df, num=new_num_points)\n",
    "\n",
    "    # reshape to fit the desired format\n",
    "    ch_1 = np.reshape(interpolated_df[:, 0], (1, -1))\n",
    "    ch_2 = np.reshape(interpolated_df[:, 1], (1, -1))\n",
    "\n",
    "    # TODO here you can augment the data! (e.g. add white noise)\n",
    "\n",
    "    # stack the data together\n",
    "    if i == 0:\n",
    "        ch_1_all = ch_1\n",
    "        ch_2_all = ch_2\n",
    "    else:\n",
    "        ch_1_all = np.vstack((ch_1_all, ch_1))\n",
    "        ch_2_all = np.vstack((ch_2_all, ch_2))\n",
    "\n",
    "    # (optional) plot data\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    # ax.plot(df[\"timestamp\"], df[\"differential_potential_CH1\"], label=\"differential_potential_CH1\")\n",
    "    # ax.plot(df[\"timestamp\"], df[\"differential_potential_CH2\"], label=\"differential_potential_CH2\")\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    # ax.plot(interpolated_df[:, 0], label=\"differential_potential_CH1\")\n",
    "    # ax.plot(interpolated_df[:, 1], label=\"differential_potential_CH2\")\n",
    "    # ax.set_xlabel(\"Time\")\n",
    "    # ax.set_ylabel(\"Voltage [mV]\")\n",
    "    # ax.legend()\n",
    "    # plt.show()\n",
    "\n",
    "# save the data\n",
    "df = pd.DataFrame(ch_1_all)\n",
    "df.to_csv(f\"data/dataset/{stimulus}/ch_1.csv\", index=False, header=False)\n",
    "df = pd.DataFrame(ch_2_all)\n",
    "df.to_csv(f\"data/dataset/{stimulus}/ch_2.csv\", index=False, header=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the classification dataset\n",
    "Now, that we have features extracted, we need to prepare the data for classification by adding class information and splitting the data in training and test set.\n",
    "\n",
    "#### Add class information\n",
    "Here we structure the data into feature vectors and the corresponding class labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load features\n",
    "\n",
    "# select which features to use\n",
    "\n",
    "\n",
    "# shuffle features\n",
    "np.random.seed(10)  # TODO set to reproduce results\n",
    "\n",
    "\n",
    "# split train and test data\n",
    "x_train = ...\n",
    "y_train = ...\n",
    "\n",
    "x_test = ...\n",
    "y_test = ...\n",
    "\n",
    "# stack the data together\n",
    "\n",
    "\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}, y_test: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Classification - classical machine learning\n",
    "In this section we will use the previous generated dataset for classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# scaling the features based on the training set\n",
    "min_max = MinMaxScaler()\n",
    "for i in range(...):  # min max scale all the features\n",
    "    pass\n",
    "# create classifier\n",
    "classifier = ...\n",
    "\n",
    "# fit classifier\n",
    "trained_classifier = ...\n",
    "\n",
    "# predict the test set\n",
    "y_pred = ...\n",
    "\n",
    "## (optinoal) Have a read into SHAP analysis for explainable\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"confusion matrix: \\n{confusion_matrix(y_test, y_pred)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Classification - deep learning\n",
    "In this section we will use the previous generated dataset for classification using deep learning. In this section I present a sample implementation of a dense neural network and LSTM network. I provide the general structure of building a model. Your task is to implement your own model using the given structure and play with the parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select parameters\n",
    "n_classes = 2  # number of classes\n",
    "np.random.seed(10)  # TODO set to reproduce results\n",
    "split_ratio = 0.8  # 80% train, 20% test\n",
    "batch_size = 32  # batch size: how many samples are used to calculate the gradient\n",
    "model_type = \"LSTM\"  # Dense or LSTM\n",
    "\n",
    "# load time series data\n",
    "\n",
    "# shuffle the data\n",
    "x_train = ...\n",
    "y_train = ...\n",
    "x_test = ...\n",
    "y_test = ...\n",
    "\n",
    "# make dim right for LSTM input data should have shape (samples, time steps, features)\n",
    "#  thereby feature corresponds to the number of channels (in our case 1 or 2\n",
    "if model_type == \"LSTM\":\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# do one hot encoding to get the probability distribution\n",
    "enc = preprocessing.OneHotEncoder(categories='auto')\n",
    "enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "# You can blur the labels for better numerical stability\n",
    "# y_train[y_train == 1] -= (0.01 * (n_classes-1))\n",
    "# y_train[y_train == 0] += 0.01\n",
    "#\n",
    "# y_test[y_test == 1] -= (0.01 * (n_classes-1))\n",
    "# y_test[y_test == 0] += 0.01\n",
    "\n",
    "# Let's make our dataset performant using tf.data API\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# build model (LSTM or Dense) or your own!\n",
    "model = Sequential()\n",
    "if model_type == \"LSTM\":\n",
    "    model.add(keras.layers.LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(keras.layers.LSTM(100))\n",
    "    model.add(keras.layers.Flatten())\n",
    "elif model_type == \"Dense\":\n",
    "    model.add(InputLayer(input_shape=(x_train.shape[1],), batch_size=batch_size))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='sigmoid'))  # see: https://stackoverflow.com/questions/49016723/softmax-cross-entropy-loss-explodes\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# check if gpu is available -> depends on your machine and is probably not available\n",
    "print(f\"is gpu available: {tf.config.list_physical_devices('GPU')}\")\n",
    "# print model summary (structure, shape, and number of parameters)\n",
    "print(model.summary())\n",
    "\n",
    "# fit model\n",
    "# save best model for later use\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"neural_network/best_{model_type}_model\",\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=100, batch_size=32,\n",
    "                    callbacks=[model_checkpoint_callback], verbose=True)\n",
    "\n",
    "# validate model -> use the best model\n",
    "model = tf.keras.models.load_model(f\"neural_network/best_{model_type}_model\")\n",
    "y_pred = model.predict(x_test, verbose=True)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# calculate metrics\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# calculate: accuracy, f1-score, and confusion matrix\n",
    "\n",
    "# plotting the loss and accuracy\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "xc = range(len(history.history['loss']))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xc, train_loss, label=\"train loss\")\n",
    "plt.plot(xc, val_loss, label=\"val loss\")\n",
    "plt.plot(xc, train_acc, label=\"train accuracy\")\n",
    "plt.plot(xc, val_acc, label=\"val accuracy\")\n",
    "plt.title(f\"{model_type} loss and accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f\"plots/{model_type}_loss_acc_.pdf\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Challenge\n",
    "Now that you have seen the basic approaches, you can try to implement your own classifier. You are allowed to use all resources. Best of luck!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO make the best classifier and win"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
